# Re-running the process to generate the notebook due to environment reset.

import nbformat as nbf

# Creating the improved notebook cells
cells_improved = []

# Preprocessing and data loading
cells_improved.append(nbf.v4.new_code_cell("""
import numpy as np
import keras
from keras.datasets import imdb
from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Embedding, GRU, Dense, Dropout, Bidirectional
from keras.regularizers import l2
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping

# Load the IMDB dataset
max_features = 20000  # Number of words to consider as features
maxlen = 100  # Cut texts after this number of words (among the max_features most common words)
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)

# Preprocessing: Pad sequences to ensure uniform input size
x_train = sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = sequence.pad_sequences(x_test, maxlen=maxlen)
"""))

# Building a more complex GRU model with bidirectional GRU layers, dropout, and L2 regularization
cells_improved.append(nbf.v4.new_code_cell("""
# Build the improved GRU model
model = Sequential()
model.add(Embedding(max_features, 128, input_length=maxlen))
model.add(Bidirectional(GRU(256, return_sequences=True, kernel_regularizer=l2(0.01))))
model.add(Dropout(0.3))
model.add(Bidirectional(GRU(256, kernel_regularizer=l2(0.01))))
model.add(Dropout(0.3))
model.add(Dense(1, activation='sigmoid'))

# Compile the model with a lower learning rate
optimizer = Adam(learning_rate=0.0001)
model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])

model.summary()  # Optional, to check the model architecture
"""))

# Early stopping and model training
cells_improved.append(nbf.v4.new_code_cell("""
# Train the model with early stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20, batch_size=32, callbacks=[early_stopping])
"""))

# Adding text generation code (optional)
cells_improved.append(nbf.v4.new_code_cell("""
# Sample text generation function
def generate_text(seed_text, next_words, model, max_sequence_length, word_index):
    for _ in range(next_words):
        token_list = keras.preprocessing.text.text_to_word_sequence(seed_text)
        token_list = [word_index[word] for word in token_list if word in word_index]
        token_list = sequence.pad_sequences([token_list], maxlen=max_sequence_length - 1, padding='pre')
        predicted = model.predict_classes(token_list, verbose=0)
        output_word = ''
        for word, index in word_index.items():
            if index == predicted:
                output_word = word
                break
        seed_text += " " + output_word
    return seed_text

# Sample usage (assuming you have word_index)
# seed_text = "Your seed text here"
# generated_text = generate_text(seed_text, 10, model, maxlen, word_index)
# print(generated_text)
"""))

# Model evaluation
cells_improved.append(nbf.v4.new_code_cell("""
# Evaluate the model
score, accuracy = model.evaluate(x_test, y_test, verbose=0)
print(f'Test score: {score:.3f}')
print(f'Test accuracy: {accuracy:.3f}')
"""))

# Creating the notebook structure and saving it
nb_improved = nbf.v4.new_notebook()
nb_improved['cells'] = cells_improved

# Write the notebook to a file
with open("/mnt/data/Improved_GRU_text_generation_analysis.ipynb", "w") as f:
    nbf.write(nb_improved, f)

"/mnt/data/Improved_GRU_text_generation_analysis.ipynb"
