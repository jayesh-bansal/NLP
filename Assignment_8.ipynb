{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jayesh-bansal/NLP/blob/main/Assignment_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment - 08"
      ],
      "metadata": {
        "id": "Mhb4AubCgqOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Example text data (you can replace this with any larger corpus) text = \"\"\" Once upon a time, there was a little girl named Red Riding Hood. She loved to visit her grandmother, who lived in the woods. One day, her mother asked her to take a basket of goodies to her grandmother. On her way through the woods, she met a big bad wolf who wanted to eat her. [CO5]\n",
        "\n",
        "(i) Build the Transformer Model on above dataset\n",
        "\n",
        "(ii) Train the model using 20, 60, 70 epochs\n",
        "\n",
        " (iii) After training, use the model to generate new text by feeding it an initial seed text\n",
        "\n",
        "(iv) Experimenting and Improving the Model by large dataset and hyper tune parameter.\n",
        "\n"
      ],
      "metadata": {
        "id": "8aPzML6_gutq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24G6xVcCcVBo"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, Dense, Dropout, LayerNormalization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Once upon a time, there was a little girl named Red Riding Hood. She loved to visit her grandmother, who lived in the woods. One day, her mother asked her to take a basket of goodies to her grandmother. On her way through the woods, she met a big bad wolf who wanted to eat her.\"\"\"\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "input_sequences = []\n",
        "for line in text.split('.'):\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "X, y = input_sequences[:,:-1], input_sequences[:,-1]\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n"
      ],
      "metadata": {
        "id": "MUisAZbnchcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential([Dense(ff_dim, activation=\"relu\"), Dense(embed_dim)])\n",
        "\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "metadata": {
        "id": "E2_XinafclBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, max_len):\n",
        "    embed_dim = 64\n",
        "    num_heads = 2\n",
        "    ff_dim = 32\n",
        "    inputs = tf.keras.Input(shape=(max_len - 1,))\n",
        "    embedding_layer = Embedding(vocab_size, embed_dim, input_length=max_len - 1)(inputs)\n",
        "\n",
        "\n",
        "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "    x = transformer_block(embedding_layer, training=True)\n",
        "\n",
        "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = Dense(20, activation=\"relu\")(x)\n",
        "    x = Dense(vocab_size, activation=\"softmax\")(x)\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
        "    return model"
      ],
      "metadata": {
        "id": "wBJ5-tj9dDVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = build_model(total_words, max_sequence_len)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "epochs_list = [20, 60, 70]\n",
        "for epochs in epochs_list:\n",
        "    print(f\"Training with {epochs} epochs...\")\n",
        "    model.fit(X, y, epochs=epochs, verbose=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "si6ctVnJdRzs",
        "outputId": "d15777fc-f400-4cd8-cfd4-bc1545c8bb78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with 20 epochs...\n",
            "Epoch 1/20\n",
            "2/2 - 4s - 2s/step - accuracy: 0.0577 - loss: 3.8884\n",
            "Epoch 2/20\n",
            "2/2 - 0s - 93ms/step - accuracy: 0.0577 - loss: 3.7060\n",
            "Epoch 3/20\n",
            "2/2 - 0s - 23ms/step - accuracy: 0.0385 - loss: 3.6694\n",
            "Epoch 4/20\n",
            "2/2 - 0s - 28ms/step - accuracy: 0.0577 - loss: 3.6360\n",
            "Epoch 5/20\n",
            "2/2 - 0s - 22ms/step - accuracy: 0.0385 - loss: 3.6540\n",
            "Epoch 6/20\n",
            "2/2 - 0s - 30ms/step - accuracy: 0.0577 - loss: 3.6275\n",
            "Epoch 7/20\n",
            "2/2 - 0s - 29ms/step - accuracy: 0.0577 - loss: 3.5746\n",
            "Epoch 8/20\n",
            "2/2 - 0s - 23ms/step - accuracy: 0.0769 - loss: 3.5675\n",
            "Epoch 9/20\n",
            "2/2 - 0s - 23ms/step - accuracy: 0.0769 - loss: 3.5697\n",
            "Epoch 10/20\n",
            "2/2 - 0s - 23ms/step - accuracy: 0.0769 - loss: 3.5276\n",
            "Epoch 11/20\n",
            "2/2 - 0s - 24ms/step - accuracy: 0.0577 - loss: 3.5382\n",
            "Epoch 12/20\n",
            "2/2 - 0s - 29ms/step - accuracy: 0.0962 - loss: 3.5083\n",
            "Epoch 13/20\n",
            "2/2 - 0s - 64ms/step - accuracy: 0.0962 - loss: 3.4765\n",
            "Epoch 14/20\n",
            "2/2 - 0s - 23ms/step - accuracy: 0.0962 - loss: 3.4570\n",
            "Epoch 15/20\n",
            "2/2 - 0s - 23ms/step - accuracy: 0.0962 - loss: 3.4744\n",
            "Epoch 16/20\n",
            "2/2 - 0s - 30ms/step - accuracy: 0.0962 - loss: 3.4264\n",
            "Epoch 17/20\n",
            "2/2 - 0s - 27ms/step - accuracy: 0.0962 - loss: 3.4542\n",
            "Epoch 18/20\n",
            "2/2 - 0s - 29ms/step - accuracy: 0.0962 - loss: 3.3677\n",
            "Epoch 19/20\n",
            "2/2 - 0s - 22ms/step - accuracy: 0.0962 - loss: 3.4183\n",
            "Epoch 20/20\n",
            "2/2 - 0s - 22ms/step - accuracy: 0.1154 - loss: 3.4188\n",
            "Training with 60 epochs...\n",
            "Epoch 1/60\n",
            "2/2 - 0s - 36ms/step - accuracy: 0.0962 - loss: 3.3410\n",
            "Epoch 2/60\n",
            "2/2 - 0s - 40ms/step - accuracy: 0.0962 - loss: 3.3423\n",
            "Epoch 3/60\n",
            "2/2 - 0s - 37ms/step - accuracy: 0.0962 - loss: 3.3037\n",
            "Epoch 4/60\n",
            "2/2 - 0s - 40ms/step - accuracy: 0.1346 - loss: 3.3055\n",
            "Epoch 5/60\n",
            "2/2 - 0s - 75ms/step - accuracy: 0.1154 - loss: 3.2726\n",
            "Epoch 6/60\n",
            "2/2 - 0s - 41ms/step - accuracy: 0.1538 - loss: 3.2841\n",
            "Epoch 7/60\n",
            "2/2 - 0s - 82ms/step - accuracy: 0.1538 - loss: 3.2355\n",
            "Epoch 8/60\n",
            "2/2 - 0s - 141ms/step - accuracy: 0.1154 - loss: 3.2300\n",
            "Epoch 9/60\n",
            "2/2 - 0s - 38ms/step - accuracy: 0.1538 - loss: 3.2141\n",
            "Epoch 10/60\n",
            "2/2 - 0s - 72ms/step - accuracy: 0.1346 - loss: 3.2025\n",
            "Epoch 11/60\n",
            "2/2 - 0s - 41ms/step - accuracy: 0.1538 - loss: 3.1840\n",
            "Epoch 12/60\n",
            "2/2 - 0s - 40ms/step - accuracy: 0.1731 - loss: 3.1544\n",
            "Epoch 13/60\n",
            "2/2 - 0s - 67ms/step - accuracy: 0.1731 - loss: 3.1292\n",
            "Epoch 14/60\n",
            "2/2 - 0s - 68ms/step - accuracy: 0.1538 - loss: 3.1123\n",
            "Epoch 15/60\n",
            "2/2 - 0s - 36ms/step - accuracy: 0.1731 - loss: 3.1246\n",
            "Epoch 16/60\n",
            "2/2 - 0s - 74ms/step - accuracy: 0.1731 - loss: 3.0817\n",
            "Epoch 17/60\n",
            "2/2 - 0s - 44ms/step - accuracy: 0.1923 - loss: 3.0787\n",
            "Epoch 18/60\n",
            "2/2 - 0s - 60ms/step - accuracy: 0.1923 - loss: 3.0612\n",
            "Epoch 19/60\n",
            "2/2 - 0s - 102ms/step - accuracy: 0.1731 - loss: 3.0747\n",
            "Epoch 20/60\n",
            "2/2 - 0s - 53ms/step - accuracy: 0.2115 - loss: 3.0488\n",
            "Epoch 21/60\n",
            "2/2 - 0s - 81ms/step - accuracy: 0.1923 - loss: 2.9942\n",
            "Epoch 22/60\n",
            "2/2 - 0s - 76ms/step - accuracy: 0.2115 - loss: 2.9643\n",
            "Epoch 23/60\n",
            "2/2 - 0s - 61ms/step - accuracy: 0.2115 - loss: 2.9637\n",
            "Epoch 24/60\n",
            "2/2 - 0s - 47ms/step - accuracy: 0.2115 - loss: 2.9642\n",
            "Epoch 25/60\n",
            "2/2 - 0s - 29ms/step - accuracy: 0.1923 - loss: 2.9286\n",
            "Epoch 26/60\n",
            "2/2 - 0s - 28ms/step - accuracy: 0.2115 - loss: 2.9108\n",
            "Epoch 27/60\n",
            "2/2 - 0s - 24ms/step - accuracy: 0.2308 - loss: 2.9380\n",
            "Epoch 28/60\n",
            "2/2 - 0s - 22ms/step - accuracy: 0.1731 - loss: 2.8893\n",
            "Epoch 29/60\n",
            "2/2 - 0s - 30ms/step - accuracy: 0.2115 - loss: 2.8331\n",
            "Epoch 30/60\n",
            "2/2 - 0s - 31ms/step - accuracy: 0.2308 - loss: 2.8119\n",
            "Epoch 31/60\n",
            "2/2 - 0s - 28ms/step - accuracy: 0.2115 - loss: 2.8159\n",
            "Epoch 32/60\n",
            "2/2 - 0s - 22ms/step - accuracy: 0.2115 - loss: 2.8213\n",
            "Epoch 33/60\n",
            "2/2 - 0s - 23ms/step - accuracy: 0.2115 - loss: 2.7502\n",
            "Epoch 34/60\n",
            "2/2 - 0s - 22ms/step - accuracy: 0.1923 - loss: 2.7576\n",
            "Epoch 35/60\n",
            "2/2 - 0s - 23ms/step - accuracy: 0.2115 - loss: 2.7686\n",
            "Epoch 36/60\n",
            "2/2 - 0s - 23ms/step - accuracy: 0.2115 - loss: 2.7151\n",
            "Epoch 37/60\n",
            "2/2 - 0s - 22ms/step - accuracy: 0.1923 - loss: 2.6733\n",
            "Epoch 38/60\n",
            "2/2 - 0s - 23ms/step - accuracy: 0.2500 - loss: 2.6916\n",
            "Epoch 39/60\n",
            "2/2 - 0s - 23ms/step - accuracy: 0.2308 - loss: 2.5861\n",
            "Epoch 40/60\n",
            "2/2 - 0s - 23ms/step - accuracy: 0.2500 - loss: 2.6245\n",
            "Epoch 41/60\n",
            "2/2 - 0s - 29ms/step - accuracy: 0.2500 - loss: 2.6123\n",
            "Epoch 42/60\n",
            "2/2 - 0s - 25ms/step - accuracy: 0.2308 - loss: 2.5785\n",
            "Epoch 43/60\n",
            "2/2 - 0s - 22ms/step - accuracy: 0.2115 - loss: 2.5415\n",
            "Epoch 44/60\n",
            "2/2 - 0s - 28ms/step - accuracy: 0.2885 - loss: 2.5343\n",
            "Epoch 45/60\n",
            "2/2 - 0s - 29ms/step - accuracy: 0.2500 - loss: 2.5147\n",
            "Epoch 46/60\n",
            "2/2 - 0s - 28ms/step - accuracy: 0.2885 - loss: 2.4659\n",
            "Epoch 47/60\n",
            "2/2 - 0s - 30ms/step - accuracy: 0.2692 - loss: 2.5007\n",
            "Epoch 48/60\n",
            "2/2 - 0s - 28ms/step - accuracy: 0.2885 - loss: 2.4378\n",
            "Epoch 49/60\n",
            "2/2 - 0s - 28ms/step - accuracy: 0.3077 - loss: 2.4570\n",
            "Epoch 50/60\n",
            "2/2 - 0s - 66ms/step - accuracy: 0.3654 - loss: 2.3629\n",
            "Epoch 51/60\n",
            "2/2 - 0s - 28ms/step - accuracy: 0.3077 - loss: 2.3668\n",
            "Epoch 52/60\n",
            "2/2 - 0s - 22ms/step - accuracy: 0.3462 - loss: 2.3273\n",
            "Epoch 53/60\n",
            "2/2 - 0s - 28ms/step - accuracy: 0.2692 - loss: 2.2787\n",
            "Epoch 54/60\n",
            "2/2 - 0s - 29ms/step - accuracy: 0.2692 - loss: 2.3217\n",
            "Epoch 55/60\n",
            "2/2 - 0s - 23ms/step - accuracy: 0.3269 - loss: 2.2616\n",
            "Epoch 56/60\n",
            "2/2 - 0s - 29ms/step - accuracy: 0.4038 - loss: 2.2154\n",
            "Epoch 57/60\n",
            "2/2 - 0s - 29ms/step - accuracy: 0.3654 - loss: 2.2034\n",
            "Epoch 58/60\n",
            "2/2 - 0s - 28ms/step - accuracy: 0.3462 - loss: 2.1671\n",
            "Epoch 59/60\n",
            "2/2 - 0s - 30ms/step - accuracy: 0.3846 - loss: 2.1625\n",
            "Epoch 60/60\n",
            "2/2 - 0s - 30ms/step - accuracy: 0.3846 - loss: 2.0330\n",
            "Training with 70 epochs...\n",
            "Epoch 1/70\n",
            "2/2 - 0s - 24ms/step - accuracy: 0.3846 - loss: 2.0635\n",
            "Epoch 2/70\n",
            "2/2 - 0s - 28ms/step - accuracy: 0.4423 - loss: 2.0084\n",
            "Epoch 3/70\n",
            "2/2 - 0s - 29ms/step - accuracy: 0.3654 - loss: 1.9584\n",
            "Epoch 4/70\n",
            "2/2 - 0s - 25ms/step - accuracy: 0.4038 - loss: 1.9581\n",
            "Epoch 5/70\n",
            "2/2 - 0s - 29ms/step - accuracy: 0.4231 - loss: 1.9356\n",
            "Epoch 6/70\n",
            "2/2 - 0s - 29ms/step - accuracy: 0.3654 - loss: 1.9031\n",
            "Epoch 7/70\n",
            "2/2 - 0s - 27ms/step - accuracy: 0.3846 - loss: 1.8586\n",
            "Epoch 8/70\n",
            "2/2 - 0s - 29ms/step - accuracy: 0.4423 - loss: 1.8486\n",
            "Epoch 9/70\n",
            "2/2 - 0s - 30ms/step - accuracy: 0.4423 - loss: 1.7545\n",
            "Epoch 10/70\n",
            "2/2 - 0s - 29ms/step - accuracy: 0.3846 - loss: 1.7812\n",
            "Epoch 11/70\n",
            "2/2 - 0s - 22ms/step - accuracy: 0.4423 - loss: 1.7586\n",
            "Epoch 12/70\n",
            "2/2 - 0s - 29ms/step - accuracy: 0.5000 - loss: 1.6832\n",
            "Epoch 13/70\n",
            "2/2 - 0s - 28ms/step - accuracy: 0.4615 - loss: 1.6932\n",
            "Epoch 14/70\n",
            "2/2 - 0s - 29ms/step - accuracy: 0.4615 - loss: 1.6913\n",
            "Epoch 15/70\n",
            "2/2 - 0s - 22ms/step - accuracy: 0.5577 - loss: 1.5797\n",
            "Epoch 16/70\n",
            "2/2 - 0s - 29ms/step - accuracy: 0.5385 - loss: 1.5731\n",
            "Epoch 17/70\n",
            "2/2 - 0s - 22ms/step - accuracy: 0.5385 - loss: 1.5487\n",
            "Epoch 18/70\n",
            "2/2 - 0s - 26ms/step - accuracy: 0.5962 - loss: 1.4437\n",
            "Epoch 19/70\n",
            "2/2 - 0s - 30ms/step - accuracy: 0.5769 - loss: 1.4693\n",
            "Epoch 20/70\n",
            "2/2 - 0s - 28ms/step - accuracy: 0.5769 - loss: 1.4865\n",
            "Epoch 21/70\n",
            "2/2 - 0s - 31ms/step - accuracy: 0.5192 - loss: 1.4205\n",
            "Epoch 22/70\n",
            "2/2 - 0s - 24ms/step - accuracy: 0.6538 - loss: 1.3802\n",
            "Epoch 23/70\n",
            "2/2 - 0s - 28ms/step - accuracy: 0.6731 - loss: 1.3631\n",
            "Epoch 24/70\n",
            "2/2 - 0s - 22ms/step - accuracy: 0.6538 - loss: 1.3759\n",
            "Epoch 25/70\n",
            "2/2 - 0s - 29ms/step - accuracy: 0.7115 - loss: 1.2852\n",
            "Epoch 26/70\n",
            "2/2 - 0s - 22ms/step - accuracy: 0.6731 - loss: 1.2942\n",
            "Epoch 27/70\n",
            "2/2 - 0s - 29ms/step - accuracy: 0.6731 - loss: 1.3002\n",
            "Epoch 28/70\n",
            "2/2 - 0s - 23ms/step - accuracy: 0.6731 - loss: 1.2494\n",
            "Epoch 29/70\n",
            "2/2 - 0s - 29ms/step - accuracy: 0.6731 - loss: 1.2245\n",
            "Epoch 30/70\n",
            "2/2 - 0s - 35ms/step - accuracy: 0.8077 - loss: 1.1931\n",
            "Epoch 31/70\n",
            "2/2 - 0s - 22ms/step - accuracy: 0.7308 - loss: 1.1461\n",
            "Epoch 32/70\n",
            "2/2 - 0s - 29ms/step - accuracy: 0.7115 - loss: 1.1633\n",
            "Epoch 33/70\n",
            "2/2 - 0s - 29ms/step - accuracy: 0.7692 - loss: 1.1246\n",
            "Epoch 34/70\n",
            "2/2 - 0s - 30ms/step - accuracy: 0.7308 - loss: 1.1476\n",
            "Epoch 35/70\n",
            "2/2 - 0s - 27ms/step - accuracy: 0.7692 - loss: 1.0608\n",
            "Epoch 36/70\n",
            "2/2 - 0s - 29ms/step - accuracy: 0.7500 - loss: 1.0458\n",
            "Epoch 37/70\n",
            "2/2 - 0s - 28ms/step - accuracy: 0.7692 - loss: 1.0230\n",
            "Epoch 38/70\n",
            "2/2 - 0s - 26ms/step - accuracy: 0.6923 - loss: 1.0687\n",
            "Epoch 39/70\n",
            "2/2 - 0s - 27ms/step - accuracy: 0.7308 - loss: 1.0328\n",
            "Epoch 40/70\n",
            "2/2 - 0s - 29ms/step - accuracy: 0.8269 - loss: 0.9624\n",
            "Epoch 41/70\n",
            "2/2 - 0s - 29ms/step - accuracy: 0.8077 - loss: 0.9170\n",
            "Epoch 42/70\n",
            "2/2 - 0s - 31ms/step - accuracy: 0.7885 - loss: 0.9663\n",
            "Epoch 43/70\n",
            "2/2 - 0s - 27ms/step - accuracy: 0.8077 - loss: 0.9663\n",
            "Epoch 44/70\n",
            "2/2 - 0s - 22ms/step - accuracy: 0.7692 - loss: 0.9396\n",
            "Epoch 45/70\n",
            "2/2 - 0s - 30ms/step - accuracy: 0.7308 - loss: 0.8992\n",
            "Epoch 46/70\n",
            "2/2 - 0s - 27ms/step - accuracy: 0.7692 - loss: 0.9090\n",
            "Epoch 47/70\n",
            "2/2 - 0s - 28ms/step - accuracy: 0.8462 - loss: 0.8074\n",
            "Epoch 48/70\n",
            "2/2 - 0s - 22ms/step - accuracy: 0.7885 - loss: 0.8407\n",
            "Epoch 49/70\n",
            "2/2 - 0s - 29ms/step - accuracy: 0.8846 - loss: 0.8665\n",
            "Epoch 50/70\n",
            "2/2 - 0s - 28ms/step - accuracy: 0.9038 - loss: 0.8330\n",
            "Epoch 51/70\n",
            "2/2 - 0s - 22ms/step - accuracy: 0.8654 - loss: 0.7685\n",
            "Epoch 52/70\n",
            "2/2 - 0s - 27ms/step - accuracy: 0.7885 - loss: 0.8005\n",
            "Epoch 53/70\n",
            "2/2 - 0s - 32ms/step - accuracy: 0.8462 - loss: 0.7406\n",
            "Epoch 54/70\n",
            "2/2 - 0s - 27ms/step - accuracy: 0.8846 - loss: 0.7046\n",
            "Epoch 55/70\n",
            "2/2 - 0s - 26ms/step - accuracy: 0.9038 - loss: 0.7366\n",
            "Epoch 56/70\n",
            "2/2 - 0s - 29ms/step - accuracy: 0.9423 - loss: 0.7023\n",
            "Epoch 57/70\n",
            "2/2 - 0s - 23ms/step - accuracy: 0.8462 - loss: 0.7267\n",
            "Epoch 58/70\n",
            "2/2 - 0s - 24ms/step - accuracy: 0.9231 - loss: 0.6654\n",
            "Epoch 59/70\n",
            "2/2 - 0s - 30ms/step - accuracy: 0.8462 - loss: 0.6938\n",
            "Epoch 60/70\n",
            "2/2 - 0s - 30ms/step - accuracy: 0.9231 - loss: 0.6001\n",
            "Epoch 61/70\n",
            "2/2 - 0s - 25ms/step - accuracy: 0.9038 - loss: 0.6199\n",
            "Epoch 62/70\n",
            "2/2 - 0s - 28ms/step - accuracy: 0.9423 - loss: 0.5964\n",
            "Epoch 63/70\n",
            "2/2 - 0s - 23ms/step - accuracy: 0.9038 - loss: 0.5888\n",
            "Epoch 64/70\n",
            "2/2 - 0s - 22ms/step - accuracy: 0.9423 - loss: 0.5801\n",
            "Epoch 65/70\n",
            "2/2 - 0s - 29ms/step - accuracy: 0.9808 - loss: 0.5316\n",
            "Epoch 66/70\n",
            "2/2 - 0s - 22ms/step - accuracy: 0.9808 - loss: 0.4930\n",
            "Epoch 67/70\n",
            "2/2 - 0s - 29ms/step - accuracy: 0.9423 - loss: 0.5304\n",
            "Epoch 68/70\n",
            "2/2 - 0s - 23ms/step - accuracy: 0.9615 - loss: 0.4695\n",
            "Epoch 69/70\n",
            "2/2 - 0s - 32ms/step - accuracy: 0.9038 - loss: 0.5281\n",
            "Epoch 70/70\n",
            "2/2 - 0s - 30ms/step - accuracy: 0.9423 - loss: 0.4631\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(seed_text, next_words, max_sequence_len):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        predicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "    return seed_text\n",
        "\n",
        "\n",
        "seed_text = \"Once upon a time\"\n",
        "print(generate_text(seed_text, 10, max_sequence_len))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JYO0MRRdvvq",
        "outputId": "0062371d-56ca-4052-b961-7a2eeee14b64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "Once upon a time there was a little girl named red riding hood hood\n"
          ]
        }
      ]
    }
  ]
}